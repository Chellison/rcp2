{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this notebook was to explore whether or not pulling ACS 5-year averages on a yearly basis would \n",
    "# improve model performance\n",
    "\n",
    "# Early results indicated \"no\" but only an average of ACS features across years was tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from matplotlib.lines import Line2D\n",
    "from shapely.geometry import Point\n",
    "import geopandas\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from scipy import stats\n",
    "from imblearn import under_sampling, over_sampling \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background')\n",
    "pd.set_option('display.max_columns',500)\n",
    "sns.set()\n",
    "\n",
    "import random\n",
    "\n",
    "SEED = 111\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/RCP2/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3062: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125792, 126)\n",
      "31448\n",
      "4    31448\n",
      "Name: NAME, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "p = Path.cwd()\n",
    "data_path = p.parent.parent / 'data' / 'Master Project Data' \n",
    "nfirs_path =  data_path / 'NFIRS Fire Incident Data.csv'\n",
    "\n",
    "# List the columns you want to download from the NFIRS csv\n",
    "cols_to_use = ['state','fdid','inc_date','oth_inj','oth_death','prop_loss',\n",
    "               'cont_loss','tot_loss','geoid']\n",
    "\n",
    "# Specify particular data type for geoid column\n",
    "col_dtypes = {'geoid':str}\n",
    "\n",
    "# Read in NFIRS dataframe\n",
    "nfirs = pd.read_csv(nfirs_path,\n",
    "                    dtype = col_dtypes,\n",
    "                    usecols = cols_to_use,\n",
    "                    encoding='latin-1')\n",
    "\n",
    "# Convert inc_date column values to python datetime type\n",
    "nfirs['inc_date'] = pd.to_datetime(nfirs['inc_date'], infer_datetime_format=True)\n",
    "\n",
    "\n",
    "#Read in recent ACS dataframes by year and state\n",
    "data_path_1 = data_path / 'Recent_ACS_Data' / 'Tennessee2013.csv'\n",
    "data_path_2 = data_path / 'Recent_ACS_Data' / 'Tennessee2014.csv'\n",
    "data_path_3 = data_path / 'Recent_ACS_Data' / 'Tennessee2015.csv'\n",
    "data_path_4 = data_path / 'Recent_ACS_Data' / 'Tennessee2016.csv'\n",
    "ACS_TN_13 = pd.read_csv(data_path_1, dtype = {'GEOID':'object'}, index_col = 2)\n",
    "ACS_TN_14 = pd.read_csv(data_path_2, dtype = {'GEOID':'object'}, index_col = 2)\n",
    "ACS_TN_15 = pd.read_csv(data_path_3, dtype = {'GEOID':'object'}, index_col = 2)\n",
    "ACS_TN_16 = pd.read_csv(data_path_4, dtype = {'GEOID':'object'}, index_col = 2)\n",
    "ACS_TN_13['year']='2013'\n",
    "ACS_TN_14['year']='2014'\n",
    "ACS_TN_15['year']='2015'\n",
    "ACS_TN_16['year']='2016'\n",
    "ACS_TN_13['state']='TN'\n",
    "ACS_TN_14['state']='TN'\n",
    "ACS_TN_15['state']='TN'\n",
    "ACS_TN_16['state']='TN'\n",
    "ACS_TN=pd.concat([ACS_TN_13, ACS_TN_14, ACS_TN_15, ACS_TN_16])\n",
    "\n",
    "\n",
    "data_path_1 = data_path / 'Recent_ACS_Data' / 'California2013.csv'\n",
    "data_path_2 = data_path / 'Recent_ACS_Data' / 'California2014.csv'\n",
    "data_path_3 = data_path / 'Recent_ACS_Data' / 'California2015.csv'\n",
    "data_path_4 = data_path / 'Recent_ACS_Data' / 'California2016.csv'\n",
    "ACS_CA_13 = pd.read_csv(data_path_1, dtype = {'GEOID':'object'}, index_col = 2)\n",
    "ACS_CA_14 = pd.read_csv(data_path_2, dtype = {'GEOID':'object'}, index_col = 2)\n",
    "ACS_CA_15 = pd.read_csv(data_path_3, dtype = {'GEOID':'object'}, index_col = 2)\n",
    "ACS_CA_16 = pd.read_csv(data_path_4, dtype = {'GEOID':'object'}, index_col = 2)\n",
    "ACS_CA_13['year']='2013'\n",
    "ACS_CA_14['year']='2014'\n",
    "ACS_CA_15['year']='2015'\n",
    "ACS_CA_16['year']='2016'\n",
    "ACS_CA_13['state']='CA'\n",
    "ACS_CA_14['state']='CA'\n",
    "ACS_CA_15['state']='CA'\n",
    "ACS_CA_16['state']='CA'\n",
    "ACS_CA=pd.concat([ACS_CA_13, ACS_CA_14, ACS_CA_15, ACS_CA_16])\n",
    "\n",
    "\n",
    "data_path_1 = data_path / 'Recent_ACS_Data' / 'Minnesota2013.csv'\n",
    "data_path_2 = data_path / 'Recent_ACS_Data' / 'Minnesota2014.csv'\n",
    "data_path_3 = data_path / 'Recent_ACS_Data' / 'Minnesota2015.csv'\n",
    "data_path_4 = data_path / 'Recent_ACS_Data' / 'Minnesota2016.csv'\n",
    "ACS_MN_13 = pd.read_csv(data_path_1, dtype = {'GEOID':'object'}, index_col = 2)\n",
    "ACS_MN_14 = pd.read_csv(data_path_2, dtype = {'GEOID':'object'}, index_col = 2)\n",
    "ACS_MN_15 = pd.read_csv(data_path_3, dtype = {'GEOID':'object'}, index_col = 2)\n",
    "ACS_MN_16 = pd.read_csv(data_path_4, dtype = {'GEOID':'object'}, index_col = 2)\n",
    "ACS_MN_13['year']='2013'\n",
    "ACS_MN_14['year']='2014'\n",
    "ACS_MN_15['year']='2015'\n",
    "ACS_MN_16['year']='2016'\n",
    "ACS_MN_13['state']='MN'\n",
    "ACS_MN_14['state']='MN'\n",
    "ACS_MN_15['state']='MN'\n",
    "ACS_MN_16['state']='MN'\n",
    "ACS_MN=pd.concat([ACS_MN_13, ACS_MN_14, ACS_MN_15, ACS_MN_16])\n",
    "\n",
    "# Create ACS dataframe for each year 13-16\n",
    "ACS_13=pd.concat([ACS_TN_13, ACS_CA_13, ACS_MN_13])\n",
    "ACS_14=pd.concat([ACS_TN_14, ACS_CA_14, ACS_MN_14])\n",
    "ACS_15=pd.concat([ACS_TN_15, ACS_CA_15, ACS_MN_15])\n",
    "ACS_16=pd.concat([ACS_TN_16, ACS_CA_16, ACS_MN_16])\n",
    "\n",
    "# Create ACS dataframe for all states and all years\n",
    "ACS=pd.concat([ACS_TN, ACS_CA, ACS_MN])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACS Munging\n",
    "\n",
    "# Ensures GEOID variable is in the correct format and sets it as the dataframe index  \n",
    "ACS_13.set_index(['GEOID'],inplace = True)\n",
    "ACS_14.set_index(['GEOID'],inplace = True)\n",
    "ACS_15.set_index(['GEOID'],inplace = True)\n",
    "ACS_16.set_index(['GEOID'],inplace = True)\n",
    "ACS_13=ACS_13.dropna(axis=0)\n",
    "\n",
    "# Captures state properies of GEOIDs for later use before filtering dataframe to be numeric features only\n",
    "States_13 = ACS_13[['state']]\n",
    "States_14 = ACS_14[['state']]\n",
    "States_15 = ACS_15[['state']]\n",
    "States_16 = ACS_16[['state']]\n",
    "\n",
    "# Removes extraneous features (i.e. non-numeric) in the dataframe\n",
    "if 'Unnamed: 0' in ACS_13.columns:\n",
    "    ACS_13.drop('Unnamed: 0','columns',inplace= True)\n",
    "if 'Unnamed: 0' in ACS_14.columns:\n",
    "    ACS_14.drop('Unnamed: 0','columns',inplace= True)\n",
    "if 'Unnamed: 0' in ACS_15.columns:\n",
    "    ACS_15.drop('Unnamed: 0','columns',inplace= True)\n",
    "if 'Unnamed: 0' in ACS_16.columns:\n",
    "    ACS_16.drop('Unnamed: 0','columns',inplace= True)\n",
    "\n",
    "\n",
    "if 'NAME' in ACS_13.columns:\n",
    "    ACS_13.drop('NAME','columns',inplace= True)\n",
    "if 'NAME' in ACS_14.columns:\n",
    "    ACS_14.drop('NAME','columns',inplace= True)\n",
    "if 'NAME' in ACS_15.columns:\n",
    "    ACS_15.drop('NAME','columns',inplace= True)\n",
    "if 'NAME' in ACS_16.columns:\n",
    "    ACS_16.drop('NAME','columns',inplace= True)\n",
    "\n",
    "\n",
    "if 'inc_pcincome' in ACS_13.columns:\n",
    "    ACS_13.drop('inc_pcincome','columns',inplace= True)\n",
    "if 'inc_pcincome' in ACS_14.columns:\n",
    "    ACS_14.drop('inc_pcincome','columns',inplace= True)\n",
    "if 'inc_pcincome' in ACS_15.columns:\n",
    "    ACS_15.drop('inc_pcincome','columns',inplace= True)\n",
    "if 'inc_pcincome' in ACS_16.columns:\n",
    "    ACS_16.drop('inc_pcincome','columns',inplace= True)\n",
    "\n",
    "    \n",
    "if 'in_poverty' in ACS_13.columns:\n",
    "    ACS_13.drop('in_poverty','columns',inplace= True)\n",
    "if 'in_poverty' in ACS_14.columns:\n",
    "    ACS_14.drop('in_poverty','columns',inplace= True)\n",
    "if 'in_poverty' in ACS_15.columns:\n",
    "    ACS_15.drop('in_poverty','columns',inplace= True)\n",
    "if 'in_poverty' in ACS_16.columns:\n",
    "    ACS_16.drop('in_poverty','columns',inplace= True)\n",
    "\n",
    "\n",
    "# Creates vector of total populations for each census block to be used to normalize total fires per year variable\n",
    "tot_pop_13 = ACS_13[['tot_population']]\n",
    "tot_pop_13_14 = pd.concat([tot_pop_13, ACS_14[['tot_population']]]).groupby(level=0).mean()\n",
    "tot_pop_13_14_15 = pd.concat([tot_pop_13, ACS_14[['tot_population']], ACS_15[['tot_population']]]).groupby(level=0).mean()\n",
    "\n",
    "\n",
    "# Drop all total count columns in ACS and keeps all percentage columns\n",
    "cols = ACS_13.columns.to_list()\n",
    "for col in cols:\n",
    "    if  col.find('tot') != -1 : \n",
    "        ACS_13.drop(col,'columns', inplace = True)\n",
    "        \n",
    "cols = ACS_14.columns.to_list()\n",
    "for col in cols:\n",
    "    if  col.find('tot') != -1 : \n",
    "        ACS_14.drop(col,'columns', inplace = True)\n",
    "        \n",
    "cols = ACS_15.columns.to_list()\n",
    "for col in cols:\n",
    "    if  col.find('tot') != -1 : \n",
    "        ACS_15.drop(col,'columns', inplace = True)\n",
    "        \n",
    "cols = ACS_16.columns.to_list()\n",
    "for col in cols:\n",
    "    if  col.find('tot') != -1 : \n",
    "        ACS_16.drop(col,'columns', inplace = True)\n",
    "\n",
    "ACS_13.drop('state','columns',inplace= True)\n",
    "ACS_14.drop('state','columns',inplace= True)\n",
    "ACS_15.drop('state','columns',inplace= True)\n",
    "ACS_16.drop('state','columns',inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30509, 12)\n",
      "(31448, 13)\n",
      "(31448, 13)\n"
     ]
    }
   ],
   "source": [
    "# Find correlated features in ACS dataset and identify the highly correlated relationships\n",
    "\n",
    "# Create ACS correlation matrix\n",
    "corr = ACS.corr()\n",
    " \n",
    "# Filtering out lower/upper triangular duplicates \n",
    "corr_high = corr[abs(corr) > 0.75].stack().reset_index()\n",
    "corr_high = corr_high[corr_high['level_0'].astype(str)!=corr_high['level_1'].astype(str)]\n",
    "corr_high['ordered-cols'] = corr_high.apply(lambda x: '-'.join(sorted([x['level_0'],x['level_1']])),axis=1)\n",
    "corr_high = corr_high.drop_duplicates(['ordered-cols'])\n",
    "corr_high.drop(['ordered-cols'], axis=1, inplace=True)\n",
    "corr_high.columns = ['Pair Var 1', 'Pair Var 2', 'Corr Value']\n",
    "\n",
    "# Display highly correlated pairs\n",
    "corr_high.sort_values(by=['Corr Value'], ascending=False)\n",
    "\n",
    "# From highly correlated pairs, remove one of the Pair Vars from the ACS dataset except for the 'mort' variables\n",
    "ACS_13 = ACS_13.drop(['house_pct_vacant', 'house_pct_non_family', 'house_pct_rent_occupied',\n",
    "                           'race_pct_nonwhite', 'race_pct_nonwhitenh', 'house_pct_incomplete_plumb',\n",
    "                           'house_pct_incomplete_kitchen', 'race_pct_whitenh', 'year'], axis=1) \n",
    "ACS_14 = ACS_14.drop(['house_pct_vacant', 'did_not_work_past_12_mo', 'house_pct_non_family', 'house_pct_rent_occupied',\n",
    "                           'race_pct_nonwhite', 'race_pct_nonwhitenh', 'house_pct_incomplete_plumb',\n",
    "                           'house_pct_incomplete_kitchen', 'race_pct_whitenh', 'year'], axis=1) \n",
    "ACS_15 = ACS_15.drop(['house_pct_vacant', 'did_not_work_past_12_mo', 'house_pct_non_family', 'house_pct_rent_occupied',\n",
    "                           'race_pct_nonwhite', 'race_pct_nonwhitenh', 'house_pct_incomplete_plumb',\n",
    "                           'house_pct_incomplete_kitchen', 'race_pct_whitenh', 'year'], axis=1) \n",
    "ACS_16 = ACS_16.drop(['house_pct_vacant', 'did_not_work_past_12_mo', 'house_pct_non_family', 'house_pct_rent_occupied',\n",
    "                           'race_pct_nonwhite', 'race_pct_nonwhitenh', 'house_pct_incomplete_plumb',\n",
    "                           'house_pct_incomplete_kitchen', 'race_pct_whitenh', 'year'], axis=1) \n",
    "\n",
    "# Based on feature importance experiments, select features with consistence importance across annual predictions\n",
    "ACS_13 = ACS_13[['house_yr_pct_earlier_1939', 'house_pct_occupied', 'house_pct_family_married', 'race_pct_black',\n",
    "          'heat_pct_fueloil_kerosene', 'educ_bachelors', 'house_pct_live_alone', \n",
    "          'educ_some_col_no_grad', 'house_pct_ownd_occupied', 'house_w_home_equity_loan', 'house_val_175K_200K',\n",
    "           'house_val_200K_250K']]\n",
    "ACS_14 = ACS_14[['house_yr_pct_earlier_1939', 'house_pct_occupied', 'house_pct_family_married', 'race_pct_black',\n",
    "          'worked_past_12_mo', 'heat_pct_fueloil_kerosene', 'educ_bachelors', 'house_pct_live_alone', \n",
    "          'educ_some_col_no_grad', 'house_pct_ownd_occupied', 'house_w_home_equity_loan', 'house_val_175K_200K',\n",
    "           'house_val_200K_250K']]\n",
    "ACS_15 = ACS_15[['house_yr_pct_earlier_1939', 'house_pct_occupied', 'house_pct_family_married', 'race_pct_black',\n",
    "          'worked_past_12_mo', 'heat_pct_fueloil_kerosene', 'educ_bachelors', 'house_pct_live_alone', \n",
    "          'educ_some_col_no_grad', 'house_pct_ownd_occupied', 'house_w_home_equity_loan', 'house_val_175K_200K',\n",
    "           'house_val_200K_250K']]\n",
    "ACS_16 = ACS_16[['house_yr_pct_earlier_1939', 'house_pct_occupied', 'house_pct_family_married', 'race_pct_black',\n",
    "          'worked_past_12_mo', 'heat_pct_fueloil_kerosene', 'educ_bachelors', 'house_pct_live_alone', \n",
    "          'educ_some_col_no_grad', 'house_pct_ownd_occupied', 'house_w_home_equity_loan', 'house_val_175K_200K',\n",
    "           'house_val_200K_250K']]\n",
    "\n",
    "# Segment ACS dataframes with moving average for successive year predictions\n",
    "ACS_13_14 = pd.concat([ACS_13, ACS_14]).groupby(level=0).mean()\n",
    "ACS_13_14_15 = pd.concat([ACS_13, ACS_14, ACS_15]).groupby(level=0).mean()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NFIRS Munging\n",
    "\n",
    "# Ensure correct calculation of tot_loss column \n",
    "nfirs['tot_loss'] = nfirs['prop_loss'] + nfirs['cont_loss']\n",
    "\n",
    "# Create mask for new severe fire variable\n",
    "sev_fire_mask = (nfirs['oth_death'] > 0) | (nfirs['oth_inj'] > 0) | (nfirs['tot_loss'] >= 10000)\n",
    "\n",
    "# By default assigns values of severe fire column as not severe\n",
    "nfirs['severe_fire'] = 'not_sev_fire'\n",
    "\n",
    "# Applies filter to severe fire column to label the severe fire instances correctly\n",
    "nfirs.loc[sev_fire_mask,'severe_fire'] = 'sev_fire'\n",
    "\n",
    "# Create new NFIRS variables based on specified thresholds of existing variables in dataframe\n",
    "nfirs['had_inj'] = np.where(nfirs['oth_inj']>0,'had_inj','no_inj')\n",
    "nfirs['had_death'] = np.where(nfirs['oth_death']>0,'had_death','no_death')\n",
    "nfirs['10k_loss'] = np.where(nfirs['tot_loss']>=10000,'had_10k_loss','no_10k_loss')\n",
    "\n",
    "# Extract just the numeric portion of the geoid\n",
    "nfirs['geoid'] =  nfirs['geoid'].str.strip('#_')\n",
    "\n",
    "# Add a year column to be used to groupby in addition to geoid\n",
    "nfirs['year'] = nfirs['inc_date'].dt.year.astype('str')\n",
    "nfirs.set_index('geoid',inplace = True)\n",
    "nfirs_CA = nfirs[nfirs['state']=='CA']\n",
    "nfirs_TN = nfirs[nfirs['state']=='TN']\n",
    "nfirs_MN = nfirs[nfirs['state']=='MN']\n",
    "nfirs=pd.concat([nfirs_CA, nfirs_TN, nfirs_MN])\n",
    "nfirs.drop('state','columns',inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adjust total fires per year by the population counts\n",
    "\n",
    "# Creates dataframe that shows the number of fires in each census block each year\n",
    "fires =  pd.crosstab(nfirs.index,nfirs['year'])\n",
    "fires.index.rename('GEOID',inplace = True)\n",
    "\n",
    "# Grab total population values pulled from ACS dataframe and assign to each census block in NFIRS dataframe\n",
    "fires1 = fires.merge(tot_pop_13, how = 'left', left_index = True, right_index = True)\n",
    "fires2 = fires.merge(tot_pop_13_14, how = 'left', left_index = True, right_index = True)\n",
    "fires3 = fires.merge(tot_pop_13_14_15, how = 'left', left_index = True, right_index = True)\n",
    "\n",
    "# Remove resulting NaN/infinity values following merge\n",
    "fires1.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "fires1.dropna(inplace = True)\n",
    "fires2.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "fires2.dropna(inplace = True)\n",
    "fires3.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "fires3.dropna(inplace = True)\n",
    "\n",
    "# drop rows with no population count\n",
    "fires1 = fires1[fires1['tot_population'] != 0 ] \n",
    "fires2 = fires2[fires2['tot_population'] != 0 ] \n",
    "fires3 = fires3[fires3['tot_population'] != 0 ] \n",
    "\n",
    "# population adjustment\n",
    "fires1.loc[:,'2010':'2014'] = fires1.loc[:,'2010':'2014'].div(fires1['tot_population'], axis = 'index') * 1000\n",
    "fires1 = fires1.loc[:,'2010':'2014']\n",
    "fires2.loc[:,'2011':'2015'] = fires2.loc[:,'2011':'2015'].div(fires2['tot_population'], axis = 'index') * 1000\n",
    "fires2 = fires2.loc[:,'2011':'2015']\n",
    "fires3.loc[:,'2012':'2016'] = fires3.loc[:,'2012':'2016'].div(fires3['tot_population'], axis = 'index') * 1000\n",
    "fires3 = fires3.loc[:,'2012':'2016']\n",
    "\n",
    "\n",
    "# view fires by year across geoids; displays additional information regarding # of fires in higher percentile categories\n",
    "fires1.describe(percentiles=[.75, .85, .9 ,.95, .99])\n",
    "fires2.describe(percentiles=[.75, .85, .9 ,.95, .99])\n",
    "fires3.describe(percentiles=[.75, .85, .9 ,.95, .99])\n",
    "\n",
    "# define variables to indicate census blocks in the top 10% percent of fire risk scores\n",
    "top10_1 = fires1 > fires1.quantile(.9)\n",
    "top10_2 = fires2 > fires2.quantile(.9)\n",
    "top10_3 = fires3 > fires3.quantile(.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upsample or downsample our dataframe features if we have unbalanced classes\n",
    "\n",
    "def resample_df(X,y,upsample=True,seed = SEED):\n",
    "    from sklearn.utils import resample\n",
    "    # check which of our two classes is overly represented \n",
    "    if np.mean(y) > .5:\n",
    "        major,minor = 1,0\n",
    "    else:\n",
    "        major,minor = 0, 1\n",
    "    \n",
    "    # Add Class feature to dataframe equal to our existing dependent variable\n",
    "    X['Class'] = y\n",
    "    \n",
    "    df_major = X[X.Class == major ]\n",
    "    df_minor = X[X.Class == minor ]\n",
    "    \n",
    "\n",
    "    if upsample:      \n",
    "    \n",
    "        df_minor_resampled = resample(df_minor,\n",
    "                                     replace = True,\n",
    "                                     n_samples = df_major.shape[0], \n",
    "                                     random_state = seed)\n",
    "    \n",
    "    \n",
    "   \n",
    "        combined = pd.concat([df_major,df_minor_resampled])\n",
    "        \n",
    "        # Debug\n",
    "        #print('minor class {}, major class {}'.format(df_minor_resampled.shape[0],\n",
    "                                                       #df_major.shape[0]))\n",
    "    \n",
    "        \n",
    "    else: # downsample\n",
    "         \n",
    "        df_major_resampled = resample(df_major,\n",
    "                                     replace = False,\n",
    "                                     n_samples = df_minor.shape[0],\n",
    "                                     random_state = seed)\n",
    "        \n",
    "        \n",
    "        combined = pd.concat([df_major_resampled,df_minor])\n",
    "        \n",
    "        #print('minor class {}, major class {}'.format(df_minor.shape[0],\n",
    "                                                      #df_major_resampled.shape[0]))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    y_out = combined['Class']\n",
    "    X_out = combined.drop('Class', axis =1)\n",
    "    return X_out , y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train model that predicts whether each census block is in the top 10% percent of fire risk scores\n",
    "def train_model(top10=pd.DataFrame(),fires=pd.DataFrame(), ACS = pd.DataFrame(), modeltype='LogisticRegression', seed = SEED):\n",
    "    from scipy.stats import zscore\n",
    "    \n",
    "    \n",
    "    # Define model types & parameters \n",
    "    \n",
    "    if modeltype =='LogisticRegression':\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        model = LogisticRegression(warm_start=True,\n",
    "                                   class_weight = 'balanced',\n",
    "                                   max_iter = 1000)\n",
    "\n",
    "        \n",
    "    elif modeltype =='BalBagged':\n",
    "        from imblearn.ensemble import BalancedBaggingClassifier\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        model = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                          n_estimators=80, sampling_strategy='auto',\n",
    "                                          random_state=0)\n",
    "        \n",
    "    elif modeltype =='BalRF':\n",
    "        from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "        model = BalancedRandomForestClassifier(n_estimators=80, sampling_strategy='auto',\n",
    "                                               max_depth=10, random_state=0,\n",
    "                                              max_features=None, min_samples_leaf=40)\n",
    "\n",
    "    elif modeltype =='Bagged':\n",
    "        from sklearn.ensemble import BaggingClassifier\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        model = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                    n_estimators=40,\n",
    "                                    random_state=0)\n",
    "    \n",
    "    elif modeltype =='RF':\n",
    "        from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "        model = BalancedRandomForestClassifier(n_estimators=60,\n",
    "                                          warm_start = False,\n",
    "                                          max_depth = 10,\n",
    "                                            random_state = 0)\n",
    "    \n",
    "    \n",
    "    # Create framework to predict whether a given census block has a fire risk score in the 90th percentile \n",
    "            # based on the specific number of previous years' data\n",
    "\n",
    "    # Should alter which \"fires#\" you use based on the model year   \n",
    "    X = fires1.iloc[:,0:4].copy()\n",
    "        \n",
    "    sm = np.sum(X, axis = 1 )\n",
    "    mu = np.mean(X, axis = 1)\n",
    "    mx = np.max(X, axis =1)\n",
    "    X['Sum']  = sm\n",
    "    X['Mean'] = mu\n",
    "    X['Max']  = mx\n",
    "    y = top10_1.iloc[:,4]\n",
    "    \n",
    "    # merge in ACS Data into X unless NFIRS-Only model\n",
    "    X=X[['Sum','Mean','Max']] # drop all other NFIRS columns that have low feature importance scores\n",
    "    # Should alter the ACS dataframe use based on the model year you want to run\n",
    "    X = X.merge(ACS_13, how ='left',left_index = True, right_index = True)\n",
    "    X = X.dropna()\n",
    "    y = y.filter(X.index)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Create 80/20 training/testing set split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .2 )\n",
    "        \n",
    "    # Perform resampling if data classes are unbalanced\n",
    "    X_train, y_train = resample_df(X_train,y_train)\n",
    "    \n",
    "    \n",
    "    # Perform cross-validation \n",
    "        \n",
    "    #scaler = preprocessing.StandardScaler().fit(X)\n",
    "    #scaler.transform(X)\n",
    "    #print ('Cross Val Score:')\n",
    "    #print(cross_val_score(model, X, y))\n",
    "        \n",
    "        \n",
    "    # Standardize features by removing the mean and scaling to unit variance\n",
    "        \n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    scaler.transform(X_train)\n",
    "    scaler.transform(X_test)\n",
    "        \n",
    "        \n",
    "    # Fit model to training set\n",
    "    model = model.fit(X_train,y_train)\n",
    "\n",
    "        \n",
    "    # Calculate training set performance\n",
    "        \n",
    "    train_prediction_probs = model.predict_proba(X_train)\n",
    "    train_predictions = model.predict(X_train)\n",
    "    print (confusion_matrix(y_train, train_predictions))\n",
    "    print (roc_auc_score(y_train, train_prediction_probs[:,1]))\n",
    "        \n",
    "        \n",
    "    # Calculate test set performance\n",
    "        \n",
    "    test_prediction_probs = model.predict_proba(X_test)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    print (confusion_matrix(y_test, test_predictions))\n",
    "    print (roc_auc_score(y_test, test_prediction_probs[:,1]))\n",
    "    print (classification_report(y_test,test_predictions))\n",
    "    print (log_loss(y_test,test_predictions))\n",
    "        \n",
    "        \n",
    "    #Calculate feature importance for each model\n",
    "        \n",
    "    if modeltype==\"LogisticRegression\":\n",
    "        feature_importance = {}\n",
    "        for coef, feat in zip(abs(model.coef_[0]),X_test.columns.tolist()):\n",
    "            feature_importance[feat] = coef\n",
    "        print(\"Feature ranking:\")\n",
    "        print (feature_importance)\n",
    "    else:\n",
    "        if modeltype==\"RF\" or modeltype==\"BalRF\":\n",
    "            importances = model.feature_importances_\n",
    "        elif modeltype==\"Bagged\":\n",
    "            importances = np.mean([model.estimators_[i].feature_importances_ for i \n",
    "                            in range(len(model.estimators_))], axis=0)\n",
    "        elif modeltype==\"BalBagged\":\n",
    "            importances = np.mean([model.estimators_[i].steps[1][1].feature_importances_ for i \n",
    "                            in range(len(model.estimators_))], axis=0)\n",
    "        \n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        print(\"Feature ranking:\")\n",
    "        for f in range(len(X_test.columns)):\n",
    "            print(\"%d. %s (%f)\" % (f + 1, X_test.columns[indices[f]], importances[indices[f]]))\n",
    "    \n",
    "    \n",
    "            \n",
    "    return model,X_test,y_test\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-43754c6ac020>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Class'] = y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16388  2983]\n",
      " [ 2962 16409]]\n",
      "0.9313336789348324\n",
      "[[3903  920]\n",
      " [ 228  326]]\n",
      "0.7883958558980696\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.81      0.87      4823\n",
      "        True       0.26      0.59      0.36       554\n",
      "\n",
      "    accuracy                           0.79      5377\n",
      "   macro avg       0.60      0.70      0.62      5377\n",
      "weighted avg       0.87      0.79      0.82      5377\n",
      "\n",
      "7.374233016736703\n",
      "Feature ranking:\n",
      "1. Max (0.429868)\n",
      "2. house_pct_live_alone (0.072803)\n",
      "3. Mean (0.060505)\n",
      "4. Sum (0.058982)\n",
      "5. house_yr_pct_earlier_1939 (0.052781)\n",
      "6. race_pct_black (0.051319)\n",
      "7. house_pct_family_married (0.050737)\n",
      "8. house_pct_occupied (0.047937)\n",
      "9. educ_bachelors (0.040676)\n",
      "10. house_pct_ownd_occupied (0.033498)\n",
      "11. educ_some_col_no_grad (0.028150)\n",
      "12. house_w_home_equity_loan (0.026294)\n",
      "13. house_val_200K_250K (0.022427)\n",
      "14. house_val_175K_200K (0.016241)\n",
      "15. heat_pct_fueloil_kerosene (0.007782)\n"
     ]
    }
   ],
   "source": [
    "# Train NFIRS + ACS Model and output prediction performance metrics for each year\n",
    "# To predict 2014 use top10_1 and ACS_13\n",
    "# To predict 2015 use top10_2 and ACS_13_14\n",
    "# To predict 2016 use top10_3 and ACS_13_14_15\n",
    "mdl,X_test,y_test =train_model(top10_1,fires1,ACS = ACS_13, modeltype='BalRF')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
